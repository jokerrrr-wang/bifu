# Bifu.co 线上问题汇总

**汇总日期:** 2025年12月2日  
**问题来源:** 生产环境监控告警与团队反馈  
**涉及服务:** trade-server, spot-trade-server, history-server, websocket-gateway, quote-server, index-server

---

## 问题分类概览

| 问题类别 | 严重等级 | 涉及服务 | 状态 |
|---------|---------|---------|------|
| 订单异常告警 | 🔴 P0 | trade-server, spot-trade-server | 待处理 |
| 订单簿数据异常 | 🔴 P0 | quote-server | 部分解决 |
| 撤单延迟 | 🔴 P0 | trade-server | 待优化 |
| 内存泄露 | 🟠 P1 | websocket-gateway | 待修复 |
| 消息堆积 | 🟠 P1 | history-server | 待优化 |
| RMQ存储容量 | 🟡 P2 | 基础设施 | 待评估 |
| Index价格不一致 | 🟡 P2 | index-server | 待修复 |
| Maker Fee负数 | 🟢 P3 | trade-server | 功能需求 |
| 慢查询问题 | 🟡 P2 | history-server | 已修复 |

---

## 一、订单异常告警处理（P0 - 致命）

### 1.1 合约liquidate异常告警

**问题描述:**
- **服务:** unimargin-trade-server-d
- **告警类型:** TradeObjectSuspendAvgDurationHigh
- **异常状态:** trade object '02.Collateral' 异常状态 '01.LIQUIDATING'
- **中断耗时:** 2h 8m 38s
- **发生时间:** 2025-09-11 08:09:00+08:00

**影响范围:**
- 强平流程阻塞超过2小时
- 用户资产处于不确定状态
- 可能导致爆仓处理延迟，造成资金损失

**根本原因:**
- 合约强平由liquidate-server处理
- trade-server等待liquidate完成时间过长

**解决方案:**
- [ ] 分析liquidate-server处理性能瓶颈
- [ ] 优化强平算法效率
- [ ] 添加强平超时保护机制（建议5分钟超时）
- [ ] 增加liquidate流程监控告警（1分钟级别）

---

### 1.2 现货订单异常告警

**问题描述:**
- **服务:** spot-trade-server-b
- **告警类型:** TradeObjectSuspendAvgDurationHigh
- **异常案例:**
  1. **Order '06.Order' 状态 '03.FILLED'** - 中断耗时 4m 7s
  2. **Order '06.Order' 状态 '04.CANCELING'** - 中断耗时 3d 3h 34m 1s
  3. **Order '06.Order' 状态 '05.CANCELED'** - 中断耗时 4m 6s
  4. **Order '07.OrderFeeIncome' 状态 '00.MATCH_OUTPUT_EVENT'** - 中断耗时 15d 0h 20m 22s

**严重性分析:**
- ❌ **最严重:** OrderFeeIncome中断耗时15天（异常）
- ❌ **严重:** CANCELING状态持续3天（异常）
- ⚠️ **中等:** FILLED/CANCELED状态4分钟（可接受但需优化）

**影响范围:**
- 订单状态机阻塞
- 撤单流程长时间挂起
- 手续费收益计算延迟15天（严重bug）
- 用户体验极差（撤单3天未完成）

**根本原因分析:**
1. **CANCELING 3天:** 
   - Raft log处理慢
   - 状态机处理队列积压
   - 可能存在死锁或资源竞争

2. **OrderFeeIncome 15天:**
   - MATCH_OUTPUT_EVENT事件处理异常
   - 可能是消息消费失败后未重试
   - RMQ消息堆积或消费者宕机

**解决方案:**
- [ ] **立即处理:** 排查OrderFeeIncome 15天问题，检查RMQ消费者状态
- [ ] **紧急修复:** 分析CANCELING 3天的订单，手动修复状态
- [ ] **架构优化:** 
  - 优化Raft log处理速度
  - 添加状态机超时保护（撤单超过5分钟强制失败）
  - 增加订单状态监控，超过1分钟告警
  - OrderFeeIncome处理异步化，避免阻塞主流程

---

## 二、history-server问题（P1 - 严重）

### 2.1 TradeEventConsumer消费问题

**问题描述:**
- TradeEventConsumer消费trade-event数据，常驻redis
- 过期处理建议：24h可行吗？
- 目前history-server里的redis统一设置了24h ttl

**解决方案:**
✅ **已解决** - 统一设置24h ttl

---

### 2.2 spot-trade-event-system的topic消息堆积

**问题描述:**
- RMQ消息堆积严重
- 图表显示消息堆积量波动，峰值约2000万条
- 消息堆积时内存占用达到峰值
- 目前rmq消息堆积已经非常大，大部分都是做市交易明细历史数据

**影响分析:**
- 历史数据查询延迟增加
- RMQ内存/磁盘压力大
- 可能影响实时消息消费

**已实施优化:**
✅ **已完成以下优化：**
- 修改topic读写队列从4改成32
- 部署4台history-server实例
- History-server的consumer代码线程数配置从64改成128

**性能指标:**
- 消费速率：从之前的每小时约150万条提升
- 消息堆积持续降低

---

### 2.3 慢查询问题

**问题描述:**
- 慢查询语句包含 `id < Long.MAX_VALUE`，导致索引失效
- 查询示例：
  ```sql
  SELECT a FROM OrderFillTransaction a 
  WHERE a.accountId = :accountId 
  AND a.contractId = contractId 
  AND id DESC
  ```
- 索引应该是：
  ```sql
  CREATE INDEX idx_position_transaction_account_contract_id
  ON t_position_transaction_xxx (accountId, contractId, id DESC);
  ```

**fixRedisData代码问题:**
- 代码里有用id<Long.MAX_VALUE，来查询，导致索引失效
- 我们暂时去掉JPA方法里的id条件查询

**解决方案:**
✅ **已修复** - 移除了id条件，使用正确的索引查询

---

### 2.4 SpotTradeEventConsumer消费问题确认

**问题描述:**
- 看了SpotTradeEventConsumer代码，使用顺序消费
- 但是设置 `rocketMQConsumeThreadMax=128`
- 对于单个队列还是串行处理的，这块可以改成并发消费吗？

**代码分析:**
```java
rocketMqConsumer.setMessageListener((MessageListenerOrderly) (msgs, context) -> {
    try {
        this.doOnMessage(msgs);
        return ConsumeOrderlyStatus.SUCCESS;
    } catch (Throwable throwable) {
        context.setSuspendCurrentQueueTimeMillis(this.isForOrder ? 10000 : 1000);
        return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;
    }
});
```

**建议方案:**
改造为并发消费：
```java
rocketMqConsumer.setMessageListener((MessageListenerConcurrently) (msgs, context) -> {
    try {
        this.doOnMessage(msgs);
        return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
    } catch (Throwable throwable) {
        log.error("[doCreateConsumerRetry] consume msg error", throwable);
        return ConsumeConcurrentlyStatus.RECONSUME_LATER;
    }
});
```

**决策:**
- [ ] 评估改为并发消费的影响
- [ ] 确认是否可以改造为并发消费

---

## 三、websocket-gateway内存泄露问题（P1 - 严重）

### 3.1 怀疑内存泄露

**问题描述:**
- 图表显示：heap memory usage after gc 值一直在增长
- 时间范围：09/02 00:00 - 09/11 00:00
- 趋势：持续上升，从约280MB涨到约460MB
- 4个实例都有相同问题

**内存增长数据:**
- 10天内增长约180MB
- 平均每天增长18MB
- 如果不处理，预计1个月后OOM

---

### 3.2 内存泄露根因

**问题代码:**
```java
// com.decode.exchange.unimargin.lizard.session.QuoteWsSessionManager#addSession
public void addSession(WebSocketSession session) {
    session.getAttributes().put(Constants.LAST_ACTIVE_TIME, System.currentTimeMillis());
    // session.getAttributes().put(Constants.PUSH_EXECUTOR, MoreExecutors.newSequentialExecutor(threadPoolExecutor)); // 这行被注释了
    sessionMap.computeIfAbsent(FULL_SESSION, String k -> new ConcurrentHashMap<>())
        .put(session.getId(), session);
}
```

**问题分析:**
- 注释掉了序列化代码（设置PUSH_EXECUTOR）
- 导致会话对象无法正确释放
- session对象在内存中累积

**影响范围:**
- 所有websocket连接都会泄露
- 随着用户连接增加，内存持续增长
- 最终导致OOM，服务宕机

**解决方案:**
- [ ] **立即修复:** 恢复被注释的代码
- [ ] **验证修复:** 部署后观察7天内存趋势
- [ ] **添加监控:** 
  - session数量监控
  - 内存增长率告警
  - after gc内存超过阈值告警（如400MB）

**修复代码:**
```java
public void addSession(WebSocketSession session) {
    session.getAttributes().put(Constants.LAST_ACTIVE_TIME, System.currentTimeMillis());
    session.getAttributes().put(Constants.PUSH_EXECUTOR, MoreExecutors.newSequentialExecutor(threadPoolExecutor)); // 恢复这行
    sessionMap.computeIfAbsent(FULL_SESSION, String k -> new ConcurrentHashMap<>())
        .put(session.getId(), session);
}
```

---

## 四、trade-server问题

### 4.1 合约负数maker fee支持（P3 - 功能需求）

**需求描述:**
- 合约想支持负数maker fee如何处理？
- 目前taker/maker fee只能设置>0的数
- 想实现通过负数maker fee来奖励补单

**业务场景:**
- 做市商补单奖励机制
- 负数fee = 平台补贴做市商
- 例如：-0.01% = 每笔交易补贴0.01%

**技术实现建议:**
1. **数据模型修改:**
   - fee字段类型改为可负数（目前可能有非负校验）
   - 修改proto定义，允许负数

2. **业务逻辑调整:**
   - 手续费计算公式支持负数
   - 资金结算流程支持补贴
   - 账户余额计算逻辑调整

3. **风控考虑:**
   - 设置负数fee下限（如-0.05%）
   - 添加补贴总额监控
   - 防止滥用机制

**优先级:** P3 - 非紧急功能需求

---

## 五、线上RMQ数据占用问题（P2 - 容量规划）

### 5.1 存储容量问题

**当前配置:**
- RMQ集群每个节点：1T硬盘
- 数据保留时间：1.11天（仅保留1天数据）
- 磁盘占用：860GB（Last 6 hours）
- 磁盘使用率：858-860GB波动

**容量分析图表:**
- 3个broker节点
- 保留的最早数据：1.11天
- 磁盘使用趋势：呈锯齿状（定期清理）

**问题:**
- 数据占用是否有问题？
- 推荐的硬盘大小是？

**容量评估:**

| 指标 | 当前值 | 建议值 | 说明 |
|------|--------|--------|------|
| 单节点容量 | 1T | 2-3T | 业务增长预留 |
| 数据保留 | 1.11天 | 3-7天 | 故障恢复需要 |
| 日均写入 | ~770GB | - | 按1.11天860GB计算 |
| 使用率 | 86% | <70% | 避免频繁清理 |

**建议方案:**

**方案一：扩容硬盘（推荐）**
- 每个节点升级到2T或3T
- 数据保留扩展到3-7天
- 成本：硬件成本增加

**方案二：优化数据保留**
- 历史数据及时归档到history-server
- 不重要的topic缩短保留时间
- 重要消息（trade-event）保留3天
- 日志类消息保留6小时

**方案三：消息压缩优化**
- 启用RocketMQ消息压缩
- 预计可节省30-50%空间
- 数据保留可延长到2-3天

**成本对比:**
- 扩容2T：¥2000/节点 × 3 = ¥6000（一次性）
- 扩容3T：¥3000/节点 × 3 = ¥9000（一次性）
- 优化方案：开发成本¥5万（人力），空间节省有限

**推荐决策:**
- ✅ **短期（1周内）:** 启用消息压缩
- ✅ **中期（1个月内）:** 扩容到2T，保留3天数据
- ✅ **长期（3个月内）:** 建立数据归档机制

---

## 六、quote-server订单簿异常问题（P0 - 致命）

### 6.1 问题现象

**问题描述:**
- 因为rmq主从切换导致trade-event消息可能丢数据
- 目前quote-server消费match-server可能存在rmq消息丢失问题
- 导致quote-server的订单簿版本号异常，行情出现无法输出
- 订单簿恢复方案最终解决方案强一致性处理，目前只是临时同步撮合最新订单簿
- 获取订单簿深度api会有500告警

**告警截图分析:**
- 规则名称：`[prod_bifu_self.trade_APT/USDT]`
- 级别状态：P0告警触发
- 触发时间：2025-10-15 02:55:48
- 监控详情：[]
- 错误信息：
  ```
  fetchPrice-ordebook failed:code: 500 message: FetchOrderBook
  failed,path:/api/v1/public/quote/getDepth
  {"code":"UNKNOWN","data":null,"msg":"unknown server internal
  error","params":null,"requestTime":"1760496946958","responseTime":"1760496948982
  ","traceId":"3f024b274f7eed465944868d617990a4"}
  ```
- 发送时间：2025-10-15 02:55:48

---

### 6.2 根本原因

**消息丢失场景:**
1. **RMQ主从切换:**
   - 主节点宕机，从节点接管
   - 切换过程中未同步的消息丢失
   - quote-server错过部分trade-event

2. **订单簿版本号不一致:**
   - match-server订单簿版本：v100
   - quote-server订单簿版本：v95（丢失5个更新）
   - 版本号校验失败，拒绝推送行情

3. **API返回500:**
   - quote-server订单簿数据异常
   - getDepth接口返回unknown server internal error

---

### 6.3 当前临时方案

**临时解决方案:**
- 订单簿恢复方案：临时同步撮合最新订单簿
- 从match-server重新拉取完整订单簿
- 重置quote-server本地版本号

**问题:**
- ❌ 只是临时方案，没有解决根本问题
- ❌ RMQ切换还会再次发生
- ❌ 手动恢复影响用户体验

---

### 6.4 最终解决方案（强一致性处理）

**方案设计:**

**1. 订单簿版本校验机制**
```java
// quote-server接收trade-event时校验版本号
if (event.orderBookVersion != localOrderBookVersion + 1) {
    // 版本号不连续，说明丢失消息
    log.error("OrderBook version mismatch, expected: {}, got: {}", 
        localOrderBookVersion + 1, event.orderBookVersion);
    
    // 触发全量同步
    syncFullOrderBookFromMatchServer();
}
```

**2. 心跳机制**
- quote-server每10秒向match-server发送心跳
- match-server返回最新订单簿版本号
- 发现版本号差异立即同步

**3. 消息持久化**
- trade-event消息持久化到数据库
- RMQ丢失时从数据库补偿
- 保证消息不丢失

**4. 降级方案**
- 订单簿异常时，API返回stale数据（标记过期）
- 而不是返回500错误
- 前端显示"行情延迟"提示

**实施计划:**
- [ ] **Week 1:** 实现版本号校验机制
- [ ] **Week 2:** 实现心跳同步机制
- [ ] **Week 3:** 实现消息持久化
- [ ] **Week 4:** 实现降级方案，灰度发布

---

## 七、线上有状态服务jraft相关问题（P1 - 性能）

### 7.1 spot-trade-server Raft状态机处理慢

**问题描述:**
- 线上spot-trade-server jraft状态机执行FSM处理偏慢
- 图表显示：每秒花费在FSM上的时间
- 时间范围：2025-10-07 15:50:00
- spot-trade-server-b：818ms
- 平均耗时：约800ms

**影响分析:**
- 导致订单处理延迟
- Raft日志应用慢
- 集群同步延迟增加

**可能原因:**
- FSM处理逻辑复杂
- 状态机处理的批量操作过大
- 磁盘IO瓶颈（日志写入）

**解决方案:**
- [ ] 分析FSM处理耗时点（使用profiler）
- [ ] 优化状态机批量处理逻辑
- [ ] 考虑异步处理非关键操作
- [ ] 升级磁盘IO性能（SSD）

**性能目标:**
- FSM处理耗时：< 100ms（优化前800ms）
- P99延迟：< 200ms

---

### 7.2 可能导致交易的raft log在状态机处理的时候被卡住

**问题描述:**
- Raft log处理被卡住
- 导致交易无法及时处理

**给出解决方案:**
1. **监控Raft处理队列深度**
   - 队列深度>1000告警
   - 处理延迟>1秒告警

2. **优化Raft批量提交**
   - 减少单批次处理数量（如1000→200）
   - 提高批次处理频率

3. **隔离关键操作**
   - 交易类操作优先级最高
   - 查询类操作可延迟处理

4. **增加Raft节点资源**
   - CPU：8核→16核
   - 内存：16GB→32GB
   - 磁盘：HDD→SSD NVMe

---

## 八、Index-Server跑两个POD时价格不一致问题（P2 - 架构缺陷）

### 8.1 问题描述

**问题现象:**
- 目前index-server单实例部署
- 如果部署多个会收到不同的indexPrice

**影响范围:**
- 无法水平扩展
- 单点故障风险
- 高可用性差

---

### 8.2 根本原因

**架构问题:**
1. **无状态服务变成有状态:**
   - 每个实例独立计算index价格
   - 计算时间差异导致价格不一致
   - 没有统一的数据源

2. **数据源不同步:**
   - 不同实例连接不同的行情源
   - 行情数据到达时间不同
   - 计算结果必然不同

---

### 8.3 解决方案

**方案一：主从架构（推荐）**
```
┌─────────────┐     ┌─────────────┐
│ index-server│ --> │ index-server│
│   (Master)  │     │   (Standby) │
└─────────────┘     └─────────────┘
      │                     │
      └──────┬──────────────┘
             ↓
      Redis (共享状态)
```

- 使用Redis选主（SETNX）
- 只有Master计算并推送价格
- Standby监听Master心跳，故障时接管

**方案二：广播模式**
- 只部署1个实例计算价格
- 计算结果广播到RMQ
- 其他服务从RMQ消费统一价格

**方案三：共享缓存**
- 所有实例从Redis读取价格
- 只有1个实例负责写入
- 使用分布式锁保证唯一性

**推荐实施方案一：**
- [ ] **Week 1:** 实现Redis选主逻辑
- [ ] **Week 2:** 实现主从切换机制
- [ ] **Week 3:** 压力测试和灰度发布
- [ ] **Week 4:** 生产环境部署2个实例

---

## 九、trade-server撤单延迟问题（P0 - 致命）

### 9.1 问题描述

**问题现象:**
- 撤做市提盘订单后订单状态存在较长时间的SUCCESS_ORDER_CANCELING
- 撤单延迟导致行情订单簿无法输出
- 订单状态机处理raft log较慢，导致rmq消息消费延迟
- 需要排查链路以及进行撤单优化

**Grafana监控数据:**
- URL: https://grafana.decodebackoffice.com/d/b875a167-12cc-46d0-9f47-720818a6c20a/tradeserver
- 图表：D8 Order 各状态数量
- [08.Order] 01.PENDING：峰值26297
- [08.Order] 02.OPEN：峰值10128
- [08.Order] 03.FILLED：峰值18353
- [08.Order] 04.CANCELING：峰值333305（异常）
- [08.Order] 05.CANCELED：18875
- [08.Order] 06.UNTRIGGERED：0

**严重性:**
- ❌ CANCELING状态订单数：333305（异常高）
- ❌ 远超其他状态订单数（正常应该<1000）
- ❌ 说明大量订单卡在撤单流程

---

### 9.2 问题截图分析

**聊天记录:**
- **Patrick Wang (10.15):** 
  - "回复 Victor Dan: @Patrick Wang 还在撤单中，没撤成功"
  - "@Victor Dan 中午的 SHIB 也还在这个状态"
  - "基本都恢复了"
  
- **Victor Dan:** 
  - "只能等着了，要等状态机处理完才行"
  - "@Victor Dan 这状态机是在用打孔纸带处理😂"

**问题确认:**
- 订单状态为CANCELING
- taker_fee_discount: "1"
- maker_fee_discount: "1"
- status: "CANCELING"
- match_sequence_id: "0"
- trigger_time: "0"
- trigger_price_time: "0"
- trigger_price_value: "0"

---

### 9.3 根本原因分析

**Raft log处理慢的原因:**

1. **状态机处理队列积压**
   - CANCELING订单33万+堆积
   - 每个订单需要等待Raft共识
   - 队列越长，延迟越大

2. **Raft写入性能瓶颈**
   - 3副本同步写入
   - 磁盘IO性能限制
   - 网络延迟累积

3. **状态机批量处理不足**
   - 单条处理效率低
   - 批量大小设置过小
   - 没有优先级队列

4. **RMQ消费延迟**
   - Raft慢导致状态更新慢
   - RMQ消息发送延迟
   - match-server无法及时更新订单簿

---

### 9.4 性能优化方案

**短期优化（1周内）:**

**1. 撤单请求批量化**
```java
// 现在：逐个撤单
for (Order order : orders) {
    cancelOrder(order.getId());  // 每个都要走Raft
}

// 优化后：批量撤单
batchCancelOrders(List<Long> orderIds);  // 一次Raft提交
```

**2. 优先级队列**
- 撤单请求优先级高于普通查询
- 快速通道处理紧急撤单
- 降低CANCELING状态停留时间

**3. 状态机异步化**
```java
// 撤单请求立即返回
// 状态机异步处理
CompletableFuture<Void> cancelOrderAsync(Long orderId) {
    return CompletableFuture.runAsync(() -> {
        // Raft状态机处理
    }, executorService);
}
```

---

**中期优化（1个月内）:**

**4. Raft性能调优**
```properties
# 增加批量提交大小
raft.apply_batch_size=500  # 默认32

# 减少选举超时（加快恢复）
raft.election_timeout_ms=1000  # 默认5000

# 优化日志复制
raft.max_replicator_inflight_msgs=256  # 默认128
```

**5. 升级硬件**
- 磁盘：HDD → SSD NVMe
  - 预计提升：IOPS 200 → 100,000+
  - 延迟：10ms → 0.1ms
- CPU：8核 → 16核
- 内存：16GB → 32GB

**6. 监控告警**
```
# 告警规则
- CANCELING订单数 > 1000：P1告警
- CANCELING订单数 > 5000：P0告警
- 撤单平均延迟 > 5秒：P1告警
- Raft队列深度 > 10000：P0告警
```

---

**长期优化（3个月内）:**

**7. 架构优化：订单状态分离**
```
┌─────────────────┐
│   trade-server  │
│  (核心交易逻辑)  │
└────────┬────────┘
         │
    ┌────┴────┐
    │  Raft   │ (强一致性状态)
    └────┬────┘
         │
┌────────┴────────┐
│  order-status   │ (订单状态缓存)
│     cache       │ (Redis，弱一致性)
└─────────────────┘
```

- 撤单操作写入Redis缓存（立即返回）
- Raft异步更新最终状态
- 降低撤单延迟：1秒 → 10ms

**8. 撤单补偿机制**
- 撤单超时5秒自动补偿
- 状态机记录未完成撤单
- 定时任务重试（每10秒）

---

### 9.5 实施计划

| 优化项 | 优先级 | 工作量 | 预期提升 | 时间 |
|--------|--------|--------|----------|------|
| 批量撤单 | P0 | 2天 | 延迟降低50% | Week 1 |
| 优先级队列 | P0 | 3天 | 延迟降低30% | Week 1 |
| Raft调优 | P1 | 2天 | 吞吐提升2x | Week 2 |
| 升级SSD | P1 | 1天 | 延迟降低90% | Week 2 |
| 监控告警 | P1 | 2天 | 及时发现问题 | Week 2 |
| 状态分离 | P2 | 2周 | 延迟降低99% | Month 1-3 |
| 补偿机制 | P2 | 1周 | 可靠性提升 | Month 1-3 |

**预期效果:**
- 短期（Week 2）：撤单延迟从5秒降低到1秒
- 中期（Month 1）：撤单延迟从1秒降低到100ms
- 长期（Month 3）：撤单延迟稳定在10ms以内

---

## 十、问题优先级排序与资源分配

### 10.1 P0级别问题（立即处理）

| 问题 | 影响 | 解决方案 | 负责人 | 截止日期 |
|------|------|----------|--------|----------|
| 订单异常告警（liquidate 2h） | 资金损失风险 | 优化liquidate流程 | 后端团队 | Week 1 |
| OrderFeeIncome 15天 | 手续费计算异常 | 排查RMQ消费者 | 后端团队 | 3天内 |
| 撤单延迟（33万订单堆积） | 用户体验极差 | 批量撤单+优先级队列 | 后端团队 | Week 1 |
| quote-server订单簿异常 | 行情无法推送 | 版本号校验+心跳同步 | 后端团队 | Week 2 |

---

### 10.2 P1级别问题（本周处理）

| 问题 | 影响 | 解决方案 | 负责人 | 截止日期 |
|------|------|----------|--------|----------|
| websocket内存泄露 | 服务宕机风险 | 恢复序列化代码 | 后端团队 | Week 1 |
| history-server消息堆积 | 查询延迟 | 已优化（确认效果） | 后端团队 | 持续监控 |
| Raft FSM处理慢 | 订单延迟 | Raft调优+升级硬件 | 后端+运维 | Week 2 |

---

### 10.3 P2级别问题（本月处理）

| 问题 | 影响 | 解决方案 | 负责人 | 截止日期 |
|------|------|----------|--------|----------|
| RMQ存储容量不足 | 数据保留短 | 扩容到2T | 运维团队 | Month 1 |
| index-server无法扩展 | 单点故障风险 | 主从架构 | 后端团队 | Month 1 |
| 慢查询问题 | 查询延迟 | 已修复（确认效果） | 后端团队 | 持续监控 |

---

### 10.4 P3级别问题（有空处理）

| 问题 | 影响 | 解决方案 | 负责人 | 截止日期 |
|------|------|----------|--------|----------|
| 负数maker fee支持 | 功能缺失 | 业务逻辑改造 | 后端+产品 | Month 2 |

---

## 十一、后续行动计划

### 11.1 本周行动（Week 1）

**Day 1-2:**
- [ ] 紧急排查OrderFeeIncome 15天问题
- [ ] 分析CANCELING 3天订单，手动修复
- [ ] 实现撤单批量化

**Day 3-5:**
- [ ] 恢复websocket序列化代码并部署
- [ ] 实现撤单优先级队列
- [ ] 优化liquidate流程超时机制

**Day 6-7:**
- [ ] 监控以上修复效果
- [ ] 准备下周优化方案

---

### 11.2 下周行动（Week 2）

- [ ] quote-server版本号校验机制
- [ ] quote-server心跳同步机制
- [ ] Raft参数调优
- [ ] 申请SSD升级方案
- [ ] 完善监控告警规则

---

### 11.3 本月行动（Month 1）

- [ ] RMQ扩容到2T
- [ ] index-server主从架构
- [ ] 撤单性能达标验证（<100ms）
- [ ] 完成所有P1问题修复

---

## 十二、总结

### 12.1 问题严重性

**致命问题（P0）：4个**
- 订单异常告警（liquidate 2h, OrderFeeIncome 15天）
- 撤单延迟（33万订单堆积）
- quote-server订单簿异常

**严重问题（P1）：3个**
- websocket内存泄露
- history-server消息堆积
- Raft FSM处理慢

**中等问题（P2）：3个**
- RMQ存储容量
- index-server扩展性
- 慢查询

**低优先级（P3）：1个**
- 负数maker fee功能

---

### 12.2 预计修复时间

- **P0问题：** 1-2周全部解决
- **P1问题：** 2-3周全部解决
- **P2问题：** 1个月内解决
- **P3问题：** 2个月内完成

---

### 12.3 资源需求

**人力需求:**
- 后端开发：3人全职投入
- 运维工程师：1人协助
- 测试工程师：1人验证

**硬件投入:**
- SSD升级：约¥3-5万
- RMQ扩容：约¥0.6-0.9万
- 总计：约¥4-6万

**预期收益:**
- 系统稳定性提升90%+
- 用户体验显著改善
- 避免资金损失风险

---

**报告生成时间:** 2025年12月2日  
**下次更新时间:** 每周一更新进度  
**责任人:** Bifu Technical Team
